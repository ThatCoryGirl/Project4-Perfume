{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8aa9f970",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Connect to the SQLite database\n",
    "conn = sqlite3.connect('perfume.db')\n",
    "\n",
    "# Load data into a DataFrame\n",
    "query = \"SELECT * FROM No_Reviews\"\n",
    "data = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Drop the 'main_accords_dirty' column\n",
    "data = data.drop(columns=['main_accords_dirty'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a098f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "def extract_main_accords(json_str):\n",
    "    try:\n",
    "        accords = ast.literal_eval(json_str)\n",
    "        if isinstance(accords, dict):\n",
    "            return ', '.join(accords.get('base', []) + accords.get('top', []) + accords.get('middle', []))\n",
    "    except (SyntaxError, ValueError):\n",
    "        pass\n",
    "    return ''\n",
    "\n",
    "data['Maine_accords'] = data['Maine_accords'].apply(extract_main_accords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7c7e23c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Categorical Columns: ['brand', 'perfume', 'notes', 'longevity', 'sillage', 'Maine_accords']\n",
      "Numerical Columns: ['launch_year']\n"
     ]
    }
   ],
   "source": [
    "# Identify the data types of each column\n",
    "data_types = data.dtypes\n",
    "\n",
    "# Separate the column names based on their data types\n",
    "categorical_cols = data_types[data_types == 'object'].index.tolist()\n",
    "numerical_cols = data_types[data_types != 'object'].index.tolist()\n",
    "\n",
    "# Print the lists of categorical and numerical columns\n",
    "print(\"Categorical Columns:\", categorical_cols)\n",
    "print(\"Numerical Columns:\", numerical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d88c746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 37925 entries, 0 to 37924\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   brand          37925 non-null  object \n",
      " 1   perfume        37922 non-null  object \n",
      " 2   launch_year    26715 non-null  float64\n",
      " 3   notes          36969 non-null  object \n",
      " 4   longevity      37925 non-null  object \n",
      " 5   sillage        37925 non-null  object \n",
      " 6   Maine_accords  37925 non-null  object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 2.0+ MB\n",
      "None\n",
      "['Rose de Grasse d Or' 'CK All' 'Clean For Men Black Leather' ...\n",
      " 'Island Blossom' 'Lemongrass Blend' 'Vanilla Blend']\n"
     ]
    }
   ],
   "source": [
    "# Check the data types\n",
    "print(data.info())\n",
    "\n",
    "# Inspect the data in the 'perfume' column\n",
    "print(data['perfume'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c49300fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand                0\n",
      "perfume              3\n",
      "launch_year      11210\n",
      "notes              956\n",
      "longevity            0\n",
      "sillage              0\n",
      "Maine_accords        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define numerical transformer\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Define categorical transformer\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine numerical and categorical transformers\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_cols),\n",
    "        ('cat', categorical_transformer, categorical_cols)\n",
    "    ])\n",
    "\n",
    "# Fit and transform transformers separately\n",
    "X_cat = categorical_transformer.fit_transform(data[categorical_cols])\n",
    "X_num = numerical_transformer.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Check for missing values\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fbabeb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Maine_accords      Another_column\n",
      "0  [0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1]  [1, 0, 1, 0, 1, 1]\n",
      "1  [1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0]  [0, 1, 0, 1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample data\n",
    "data = {\n",
    "    'Maine_accords': [\n",
    "        \"woody, musky, leather, patchouli, rose, oud\",\n",
    "        \"amber, citrus, white floral, animalic, balsamic\"\n",
    "    ],\n",
    "    'Another_column': [\n",
    "        \"another, example, of, text\",\n",
    "        \"more, text, data\"\n",
    "    ]\n",
    "    # Add more columns as needed\n",
    "}\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "count_vectorizer = CountVectorizer(min_df=1) \n",
    "\n",
    "# Initialize an empty DataFrame to store the transformed data\n",
    "X_bow_all = pd.DataFrame()\n",
    "\n",
    "# Loop through each column in the DataFrame\n",
    "for column in df.columns:\n",
    "    # Extract the text data from the column\n",
    "    text_data = df[column]\n",
    "    \n",
    "    # Fit and transform the text data\n",
    "    X_bow = count_vectorizer.fit_transform(text_data)\n",
    "    \n",
    "    # Add the transformed data to X_bow_all DataFrame\n",
    "    X_bow_all[column] = X_bow.toarray().tolist()\n",
    "\n",
    "# Print the transformed data\n",
    "print(X_bow_all)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5b646777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1\n",
      "Testing set size: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into features (X) and target labels (y)\n",
    "X = X_bow_all.drop(columns=['Maine_accords'])\n",
    "y = df['Maine_accords']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape[0])\n",
    "print(\"Testing set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4ded587",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'woody, musky, leather, patchouli, rose, oud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m logistic_regression_model \u001b[38;5;241m=\u001b[39m LogisticRegression()\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model on the training data\u001b[39;00m\n\u001b[1;32m---> 30\u001b[0m logistic_regression_model\u001b[38;5;241m.\u001b[39mfit(X_train_encoded, y_train)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the testing data\u001b[39;00m\n\u001b[0;32m     33\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m logistic_regression_model\u001b[38;5;241m.\u001b[39mscore(X_test_encoded, y_test)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1252\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1250\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1252\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1253\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1255\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1256\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1257\u001b[0m     )\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1260\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 'woody, musky, leather, patchouli, rose, oud'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Split the data into features (X) and target labels (y)\n",
    "X = X_bow_all.drop(columns=['Maine_accords'])\n",
    "y = df['Maine_accords']\n",
    "\n",
    "# Split the data into training and testing sets (80% train, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "def one_hot_encode(data):\n",
    "    num_samples, num_features = data.shape\n",
    "    encoded_data = np.zeros((num_samples, num_features * 2), dtype=int)\n",
    "    for i in range(num_samples):\n",
    "        for j in range(num_features):\n",
    "            value = data[i, j]\n",
    "            if isinstance(value, list):\n",
    "                value = int(value[0])  # Convert the list to an integer\n",
    "            encoded_data[i, j * 2 + value] = 1\n",
    "    return encoded_data\n",
    "\n",
    "X_train_encoded = one_hot_encode(X_train.to_numpy())\n",
    "X_test_encoded = one_hot_encode(X_test.to_numpy())\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "\n",
    "# Train the model on the training data\n",
    "logistic_regression_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# Evaluate the model on the testing data\n",
    "accuracy = logistic_regression_model.score(X_test_encoded, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1909cf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
